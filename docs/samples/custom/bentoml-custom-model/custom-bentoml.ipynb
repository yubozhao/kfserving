{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on a InferenceService with BentoML\n",
    "\n",
    "\n",
    "The notebook shows how to deploy and make predict against a KFServing InferenceService with BentoML. [BentoML](https://bentoml.org) is an open-source platform for high-performance ML model serving, which supports all major machine learning frameworks including Keras, Tensorflow, PyTorch, Fast.ai, XGBoost and etc.\n",
    "\n",
    "\n",
    "In this notebook, it will trains a classification model with the iris data set, packages with BentoML, and then deploys to KFserving installed cluster for inferencing.\n",
    "\n",
    "\n",
    "### Setup\n",
    "\n",
    "* Your ~/.kube/config should point to a cluster with KFServing installed.\n",
    "* Your cluster's Istio Ingress gateway must be network accessible.\n",
    "* Docker and Docker hub must be properly configured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train and save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn import datasets\n",
    "\n",
    "\n",
    "# Load training data\n",
    "iris = datasets.load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "# Model Training\n",
    "clf = svm.SVC(gamma='scale')\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Define ML service with BentoML**\n",
    "\n",
    "\n",
    "BentoML creates a model API server, via prediction service abstraction.\n",
    "\n",
    "The following code will be saved to a file name `iris_classifier.py`. It defines a prediction service that requires a scikit-learn model, and asks BentoML to figure out the required PyPI pip packages automatically. It also defined an API, which is the entry point for accessing this prediction service. The API is expecting a pandas.DataFrame object as its input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile iris_classifier.py\n",
    "\n",
    "from bentoml import env, artifacts, api, BentoService\n",
    "from bentoml.handlers import DataframeHandler\n",
    "from bentoml.artifact import SklearnModelArtifact\n",
    "\n",
    "\n",
    "@env(auto_pip_dependencies=True)\n",
    "@artifacts([SklearnModelArtifact('model')])\n",
    "class IrisClassifier(BentoService):\n",
    "\n",
    "    @api(DataframeHandler)\n",
    "    def predict(self, df):\n",
    "        return self.artifacts.model.predict(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the trained model to local disk with the BentoML prediction service defined above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from iris_classifier import IrisClassifier\n",
    "\n",
    "# Create a iris classifier service instance\n",
    "iris_classifier_service = IrisClassifier()\n",
    "\n",
    "# Pack the newly trained model artifact\n",
    "iris_classifier_service.pack('model', clf)\n",
    "\n",
    "# Save the prediction service to disk for model serving\n",
    "saved_path = iris_classifier_service.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validate prediction result with sample data using BentoML CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!bentoml run IrisClassifier:latest predict --input '[[5.1, 3.5, 1.4, 0.2]]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy custom InferenceService\n",
    "\n",
    "\n",
    "BentoML's REST interface is different than the Tensorflow V1 HTTP API that KFServing expects.  Requests will send directly to the prediction service and bypass the top level inferenceservice. \n",
    "\n",
    "*Note: Support for KFserving V2 prediction protocol with BentoML is coming soon.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BentoML automatically generates a Dockerfile for API server when saving model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Replace DOCKER_USERNAME with the Docker Hubb username\n",
    "docker_username=DOCKER_USERNAME\n",
    "model_path=$(bentoml get IrisClassifier:latest -q | jq -r \".uri.uri\")\n",
    "\n",
    "docker build -t $docker_username/iris-classifier $model_path\n",
    "\n",
    "docker push $docker_username/iris-classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Update the docker username inside InferenceServer configuration and apply to the cluster*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "# Replace DOCKER_USERNAME with the Docker Hub username\n",
    "docker_username=DOCKER_USERNAME \n",
    "\n",
    "sed -i 's/{docker_username}/'\"$docker_username\"'/g' custom.yaml\n",
    "\n",
    "kubectl apply -f custom.yaml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run prediction\n",
    "\n",
    "*Note: Use kfserving-ingressgateway as your INGRESS_GATEWAY if you are deploying KFServing as part of Kubeflow install, and not independently.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "MODEL_NAME=iris-classifier\n",
    "INGRESS_GATEWAY=istio-ingressgateway\n",
    "CLUSTER_IP=$(kubectl -n istio-system get service $INGRESS_GATEWAY -o jsonpath='{.status.loadBalancer.ingress[0].ip}')\n",
    "SERVICE_HOSTNAME=$(kubectl get inferenceservice ${MODEL_NAME} -o jsonpath='{.status.url}' | cut -d \"/\" -f 3)\n",
    "\n",
    "curl -v -H \"Host: ${SERVICE_HOSTNAME}\" \\\n",
    "  --header \"Content-Type: application/json\" \\\n",
    "  --request POST \\\n",
    "  --data '[[5.1, 3.5, 1.4, 0.2]]' \\\n",
    "  http://$CLUSTER_IP/model/predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!kubectl delete -f custom.yaml"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
